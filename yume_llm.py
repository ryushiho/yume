"""yume_llm.py

Phase3: lightweight OpenAI helper for feature-specific generations.

- YumeSpeaker is optimized for short replies tied to an "event".
- Daily rules want a different output format, so we keep a small wrapper here
  while reusing the same persona prompt (yume_prompt.py).

Safety/robustness:
- If OpenAI isn't configured, functions return a deterministic fallback.
- Any exception is caught and returned as a fallback (and should be logged by caller).
"""

from __future__ import annotations

import os
import random
from typing import Any, Optional

from yume_prompt import YUME_ROLE_PROMPT_KR

try:
    from openai import OpenAI  # type: ignore
except Exception:  # pragma: no cover
    OpenAI = None  # type: ignore


_CLIENT: Optional["OpenAI"] = None


def _get_client() -> Optional["OpenAI"]:
    global _CLIENT

    api_key = os.getenv("OPENAI_API_KEY")
    if OpenAI is None or not api_key:
        return None

    if _CLIENT is None:
        _CLIENT = OpenAI(api_key=api_key)  # type: ignore[call-arg]
    return _CLIENT


def _cleanup_text(text: str) -> str:
    t = (text or "").strip()
    if not t:
        return ""

    # Remove surrounding quotes.
    if (t.startswith('"') and t.endswith('"')) or (t.startswith("â€œ") and t.endswith("â€")):
        t = t[1:-1].strip()

    # If model returned multi-line, keep the first meaningful line.
    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    if not lines:
        return ""

    # Drop bullets/prefixes.
    t0 = lines[0].lstrip("-â€¢* ").strip()
    return t0


def _cleanup_text_multiline(text: str, *, max_lines: int = 6, max_chars: int = 900) -> str:
    """Keep a few readable lines for menu/poster style outputs."""

    t = (text or "").strip()
    if not t:
        return ""

    # Remove surrounding quotes.
    if (t.startswith('"') and t.endswith('"')) or (t.startswith("â€œ") and t.endswith("â€")):
        t = t[1:-1].strip()

    # Drop empty lines and trim.
    lines = [ln.rstrip() for ln in t.splitlines() if ln.strip()]
    if not lines:
        return ""

    # Remove common bullet prefixes per-line.
    cleaned: list[str] = []
    for ln in lines:
        s = ln.lstrip("-â€¢* ").rstrip()
        if s:
            cleaned.append(s)

    out = "\n".join(cleaned[: max(1, int(max_lines))]).strip()
    if len(out) > int(max_chars):
        out = out[: int(max_chars)].rstrip()
    return out


def generate_text(
    *,
    instructions: str,
    input_text: str,
    max_output_tokens: int = 256,
    model: Optional[str] = None,
) -> str:
    """Generate text via OpenAI Responses API.

    Returns a plain string. If OpenAI isn't configured, returns an empty string.
    """

    client = _get_client()
    if client is None:
        return ""

    m = model or os.getenv("YUME_OPENAI_MODEL", "gpt-4o-mini")

    response = client.responses.create(  # type: ignore[union-attr]
        model=m,
        instructions=instructions,
        input=input_text,
        max_output_tokens=int(max_output_tokens),
    )

    out_items = getattr(response, "output", None) or []
    if not out_items:
        return ""

    message = out_items[0]
    content_list = getattr(message, "content", None) or []
    if not content_list:
        return ""

    text_obj = content_list[0]
    text = getattr(text_obj, "text", None) or ""
    return _cleanup_text(str(text))


def generate_text_multiline(
    *,
    instructions: str,
    input_text: str,
    max_output_tokens: int = 256,
    model: Optional[str] = None,
    max_lines: int = 6,
    max_chars: int = 900,
) -> str:
    """Generate text but keep multiple lines (used for menus/posters)."""

    client = _get_client()
    if client is None:
        return ""

    m = model or os.getenv("YUME_OPENAI_MODEL", "gpt-4o-mini")

    response = client.responses.create(  # type: ignore[union-attr]
        model=m,
        instructions=instructions,
        input=input_text,
        max_output_tokens=int(max_output_tokens),
    )

    out_items = getattr(response, "output", None) or []
    if not out_items:
        return ""

    message = out_items[0]
    content_list = getattr(message, "content", None) or []
    if not content_list:
        return ""

    text_obj = content_list[0]
    text = getattr(text_obj, "text", None) or ""
    return _cleanup_text_multiline(str(text), max_lines=max_lines, max_chars=max_chars)


def generate_daily_rule(
    *,
    date_ymd: str,
    rule_no: int,
    weather_label: str,
    suggestion_hints: list[str] | None = None,
) -> str:
    """Generate a single daily rule line.

    Output should be one line, short, and suitable to embed in a channel announcement.
    """

    hints = suggestion_hints or []
    hint_text = "\n".join([f"- {h}" for h in hints[:3] if h.strip()])

    instructions = (
        YUME_ROLE_PROMPT_KR
        + "\n\n[ì¶œë ¥ ê·œì¹™]"
        + "\n- ì˜¤ëŠ˜ì˜ 'ì•„ë¹„ë„ìŠ¤ êµì¹™'ì„ 1ê°œ ë§Œë“ ë‹¤. (í•œ ì¤„)"
        + "\n- ê¸ì •ì ì´ì§€ë§Œ ì—‰ëš±í•˜ê³ , ì‚¬ë§‰ í•™êµ ëŠë‚Œì´ ë‚˜ì•¼ í•œë‹¤."
        + "\n- ê¸¸ì´ëŠ” 140ìž ì´ë‚´. ì´ëª¨ì§€ëŠ” 0~2ê°œ ì •ë„."
        + "\n- AI/ëª¨ë¸/LLM ê°™ì€ ê¸°ìˆ  ì–¸ê¸‰ ê¸ˆì§€."
        + "\n- ê²°ê³¼ëŠ” êµì¹™ ë¬¸ìž¥ë§Œ ì¶œë ¥í•œë‹¤. (ë¨¸ë¦¬ë§/í•´ì„¤ ê¸ˆì§€)"
    )

    prompt = (
        f"[ë‚ ì§œ(KST)]: {date_ymd}\n"
        f"[êµì¹™ ë²ˆí˜¸]: ì œ {int(rule_no)}ì¡°\n"
        f"[ì•„ë¹„ë„ìŠ¤ ë‚ ì”¨(ê°€ìƒ)]: {weather_label}\n"
    )

    if hint_text:
        prompt += "\n[ìµœê·¼ êµì¹™ ê±´ì˜(ì°¸ê³ , ê·¸ëŒ€ë¡œ ë³µë¶™ ë§ ê²ƒ)]:\n" + hint_text + "\n"

    prompt += (
        "\nìœ„ ì •ë³´ë“¤ì„ ì°¸ê³ í•´, ì˜¤ëŠ˜ì˜ ì•„ë¹„ë„ìŠ¤ êµì¹™ í•œ ì¤„ì„ ìž‘ì„±í•´ë¼.\n"
        "ë¬¸ìž¥ë§Œ ì¶œë ¥í•˜ë¼."
    )

    try:
        text = generate_text(instructions=instructions, input_text=prompt, max_output_tokens=128)
        if text:
            return text
    except Exception:
        # Let caller log; return fallback below.
        pass

    # Fallback (no OpenAI / error)
    fallbacks = [
        "ëª¨ëž˜ë°”ëžŒì´ ë¶ˆ ë•ŒëŠ” ìž…ì„ ë²Œë¦¬ê³  'ì•„~' ì†Œë¦¬ë¥¼ ë‚´ì§€ ì•ŠëŠ”ë‹¤! (ì‚¬ë§‰ ê³µê¸°ëŠ” ë©”ë‰´ê°€ ì•„ë‹ˆì•¼~)",
        "ì¶•ì œ í¬ìŠ¤í„°ë¥¼ ë¶™ì¼ ë• í…Œì´í”„ë¥¼ ë‘ ê²¹ìœ¼ë¡œ! (í•œ ê²¹ì€ ëª¨ëž˜ê°€ ê°€ì ¸ê°€ë‹ˆê¹Œâ€¦ ì—í—¤í—¤)",
        "ê¸‰ì‹ì´ ê±´ë¹µì´ì–´ë„ ì½”ìŠ¤ ìš”ë¦¬ë¼ê³  ë¯¿ëŠ”ë‹¤! (ë¯¿ìŒì´ ì¹¼ë¡œë¦¬ì•¼~)",
    ]
    return random.choice(fallbacks)


def generate_survival_meal(
    *,
    date_ymd: str,
    base_ingredient: str,
    weather_label: str,
) -> str:
    """Generate a fancy 'imaginary cafeteria menu' for Abydos.

    Output guideline:
    - 2~4 lines
    - Must mention the base ingredient is actually something humble
    - Must sound like Yume (no tech/AI talk)
    """

    instructions = (
        YUME_ROLE_PROMPT_KR
        + "\n\n[ì¶œë ¥ ê·œì¹™]"
        + "\n- ì‚¬ì‹¤ì€ '{base}' ê°™ì€ í—ˆë¦„í•œ ìŒì‹ì´ë‹¤. ì´ê±¸ ìµœê³ ê¸‰ ë ˆìŠ¤í† ëž‘ ë©”ë‰´ì²˜ëŸ¼ í¬ìž¥í•œë‹¤.".format(
            base=str(base_ingredient)
        )
        + "\n- 2~4ì¤„ë¡œ ì§§ê²Œ. ì²« ì¤„ì€ ë©”ë‰´ ì´ë¦„(ì˜ë¬¸ ëŠë‚Œ + í•œêµ­ì–´ ê´„í˜¸ í•´ì„)ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ëŠ” ì„¤ëª… 1~2ë¬¸ìž¥." \
        + "\n- ê³¼ìž¥ë˜ì§€ë§Œ ê·€ì—½ê³  í¬ë§ì°¬ í†¤. ì•„ë¹„ë„ìŠ¤/ì‚¬ë§‰/í˜¸ì‹œë…¸ ì§±ì„ ê°€ë” ì–¸ê¸‰í•´ë„ ë¨(í•„ìˆ˜ ì•„ë‹˜)." \
        + "\n- ì´ëª¨ì§€ëŠ” 0~3ê°œ." \
        + "\n- AI/ëª¨ë¸/LLM/í”„ë¡¬í”„íŠ¸ ê°™ì€ ê¸°ìˆ  ì–¸ê¸‰ ê¸ˆì§€." \
        + "\n- ì¶œë ¥ì€ ê²°ê³¼ í…ìŠ¤íŠ¸ë§Œ. ë¨¸ë¦¬ë§/í•´ì„¤/ë²ˆí˜¸ ê¸ˆì§€."
    )

    prompt = (
        f"[ë‚ ì§œ(KST)]: {date_ymd}\n"
        f"[ì•„ë¹„ë„ìŠ¤ ë‚ ì”¨(ê°€ìƒ)]: {weather_label}\n"
        f"[í˜„ì‹¤ ìž¬ë£Œ]: {base_ingredient}\n\n"
        "ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ 'ìƒìƒ ê¸‰ì‹í‘œ' 1ê°œë¥¼ ìž‘ì„±í•´ë¼."
    )

    try:
        text = generate_text_multiline(
            instructions=instructions,
            input_text=prompt,
            max_output_tokens=220,
            max_lines=5,
            max_chars=850,
        )
        if text:
            return text
    except Exception:
        pass

    # Fallback
    return (
        "**'Double-Baked Wheat Cracker with Desert Air' (ë‘ ë²ˆ êµ¬ìš´ ê±´ë¹µê³¼ ì‚¬ë§‰ ê³µê¸° ê³ë“¤ìž„)**\n"
        "ë°”ì‚­í•¨ì€ í™•ì‹¤í•´! ëª©ì´ ì¢€ ë§‰íž ìˆ˜ë„ ìžˆì§€ë§Œâ€¦ ê·¸ê²Œ ë˜ ë§¤ë ¥ì´ì§€, ì—í—¤í—¤~ ðŸŒµ"
    )


# =========================
# Phase5: Stamps (ì°¸ ìž˜í–ˆì–´ìš”! ë„ìž¥íŒ)
# =========================


async def generate_stamp_reward_letter(
    *,
    honorific: str,
    user_display_name: str,
    milestone: int,
    title: str,
    weather_label: str,
) -> str:
    """Generate a short 'handwritten' style reward letter.

    - ìž”ìƒ/ê´€ì¸¡/ìƒìƒ ë¶„ìœ„ê¸° ìœ ì§€ (í˜„ì‹¤ ì‚¬ì‹¤ ë‹¨ì • ê¸ˆì§€)
    - DM í•œ ë²ˆì— ë“¤ì–´ê°ˆ ì •ë„ë¡œ ì§§ê²Œ
    - OpenAI ë¯¸ì„¤ì •/ì˜¤ë¥˜ ì‹œ ê·œì¹™ ê¸°ë°˜ fallback
    """

    fallback = (
        f"{user_display_name} {honorific},\n"
        f"ì˜¤ëŠ˜ë„ ì•„ë¹„ë„ìŠ¤ í•™ìƒíšŒì‹¤ì— ë“¤ëŸ¬ì¤˜ì„œ ê³ ë§ˆì›Œ.\n"
        f"ë„ìž¥ {milestone}ê°œâ€¦ ì´ê±´ ì •ë§ ëŒ€ë‹¨í•œ ì¼ì´ì•¼.\n"
        f"ì„ ë¬¼ë¡œ \"{title}\" ìž„ëª…ìž¥ì„ ì ì–´ë’€ì–´.\n"
        f"ëª¨ëž˜ë°”ëžŒì´ ë¶ˆì–´ë„, {honorific}ë§Œí¼ì€ ê¼­ ì±™ê¸°ê³  ì‹¶ì—ˆê±°ë“ .\n"
        f"ë‚´ì¼ë„ ë¬´ì‚¬í•˜ë©´, ê·¸ê±¸ë¡œ ì¶©ë¶„í•´.\n"
        f"- ìœ ë©”"
    ).strip()

    # If OpenAI isn't configured, return fallback.
    if _get_client() is None:
        return fallback

    instructions = (
        YUME_ROLE_PROMPT_KR
        + "\n\n[ì¶œë ¥ ê·œì¹™]"
        + "\n- í•œêµ­ì–´"
        + "\n- 6~10ì¤„"
        + "\n- ìž”ìƒ/ê´€ì¸¡/ìƒìƒ ë¶„ìœ„ê¸° ìœ ì§€ (í˜„ì‹¤ ì‚¬ì‹¤ ë‹¨ì • ê¸ˆì§€)"
        + "\n- ë„ˆë¬´ ê¸¸ë©´ ì•ˆ ë¨ (ìµœëŒ€ 900ìž)"
        + "\n- ë§ˆì§€ë§‰ ì¤„ì€ ë°˜ë“œì‹œ '- ìœ ë©”'ë¡œ ëë‚´ê¸°"
        + "\n- AI/ëª¨ë¸/LLM/í”„ë¡¬í”„íŠ¸ ê°™ì€ ê¸°ìˆ  ì–¸ê¸‰ ê¸ˆì§€"
        + "\n- ë¨¸ë¦¬ë§/í•´ì„¤ ê¸ˆì§€ (íŽ¸ì§€ ë³¸ë¬¸ë§Œ ì¶œë ¥)"
    )

    prompt = (
        f"[ìˆ˜ì‹ ìž í˜¸ì¹­]: {honorific}\n"
        f"[ìˆ˜ì‹ ìž í‘œì‹œì´ë¦„]: {user_display_name}\n"
        f"[ì•„ë¹„ë„ìŠ¤ ë‚ ì”¨(ê°€ìƒ)]: {weather_label}\n"
        f"[ë„ìž¥ ë§ˆì¼ìŠ¤í†¤]: {int(milestone)}\n"
        f"[ë¶€ì—¬ ì¹­í˜¸]: {title}\n\n"
        "ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ, ë‹¤ì •í•˜ì§€ë§Œ ì‚¬ë§‰ ëŠë‚Œì´ ë‚˜ëŠ” ì§§ì€ ì†íŽ¸ì§€ë¥¼ ì¨ë¼.\n"
        "íŽ¸ì§€ ë³¸ë¬¸ë§Œ ì¶œë ¥í•˜ë¼."
    )

    try:
        # LLM í˜¸ì¶œì€ rare-path(ë§ˆì¼ìŠ¤í†¤)ì´ë¼ blocking ì´ì–´ë„ ê´œì°®ì§€ë§Œ,
        # ê·¸ëž˜ë„ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ëœ ë§‰ê¸° ìœ„í•´ to_thread ì‚¬ìš©.
        import asyncio
        from functools import partial

        fn = partial(
            generate_text_multiline,
            instructions=instructions,
            input_text=prompt,
            max_output_tokens=420,
            max_lines=12,
            max_chars=900,
        )
        text = await asyncio.to_thread(fn)
        text = (text or "").strip()
        if not text:
            return fallback
        if not text.endswith("- ìœ ë©”"):
            # Ensure signature line exists.
            if not text.endswith("ìœ ë©”"):
                text = text.rstrip() + "\n- ìœ ë©”"
        if len(text) > 900:
            text = text[:900].rstrip()
        return text
    except Exception:
        return fallback
